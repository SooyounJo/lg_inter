import { useState, useRef, useEffect } from 'react'

// í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ í›… (ëª¨ë°”ì¼: Web Speech API, ë°ìŠ¤í¬í†±: Google Cloud)
export const useVoiceRecognition = (onResult, onError) => {
  const [isListening, setIsListening] = useState(false)
  const [isMobile, setIsMobile] = useState(false)
  
  // Web Speech API (ëª¨ë°”ì¼ìš©)
  const recognitionRef = useRef(null)
  
  // MediaRecorder (ë°ìŠ¤í¬í†± Google Cloudìš©)
  const mediaRecorderRef = useRef(null)
  const audioChunksRef = useRef([])
  const streamRef = useRef(null)

  // ëª¨ë°”ì¼ ê°ì§€
  useEffect(() => {
    const checkMobile = () => {
      const mobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)
      setIsMobile(mobile)
      
      // ëª¨ë°”ì¼ì´ë©´ Web Speech API ì´ˆê¸°í™”
      if (mobile && typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
        const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition
        recognitionRef.current = new SpeechRecognition()
        recognitionRef.current.continuous = false
        recognitionRef.current.interimResults = false
        recognitionRef.current.lang = 'ko-KR'

        recognitionRef.current.onresult = (event) => {
          const transcript = event.results[0][0].transcript
          if (transcript) {
            onResult(transcript)
          }
          setIsListening(false)
        }

        recognitionRef.current.onerror = (error) => {
          console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', error)
          setIsListening(false)
          onError && onError(error)
        }

        recognitionRef.current.onend = () => {
          setIsListening(false)
        }
      }
    }
    
    checkMobile()
  }, [onResult, onError])

  // ìŒì„± ì¸ì‹ ì‹œìž‘ (ëª¨ë°”ì¼: Web Speech API)
  const startListeningMobile = () => {
    if (!recognitionRef.current) {
      onError && onError(new Error('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'))
      return false
    }

    try {
      recognitionRef.current.start()
      setIsListening(true)
      return true
    } catch (error) {
      console.error('ìŒì„± ì¸ì‹ ì‹œìž‘ ì˜¤ë¥˜:', error)
      onError && onError(error)
      return false
    }
  }

  // ìŒì„± ì¸ì‹ ì‹œìž‘ (ë°ìŠ¤í¬í†±: Google Cloud)
  const startListeningDesktop = async () => {
    try {
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: true
      })
      
      streamRef.current = stream
      audioChunksRef.current = []

      // MediaRecorder ì„¤ì •
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
        ? 'audio/webm;codecs=opus' 
        : 'audio/webm'
      
      mediaRecorderRef.current = new MediaRecorder(stream, { mimeType })

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        // ì˜¤ë””ì˜¤ ë¸”ë¡­ ìƒì„±
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
        
        // Base64ë¡œ ë³€í™˜
        const reader = new FileReader()
        reader.readAsDataURL(audioBlob)
        reader.onloadend = async () => {
          const base64Audio = reader.result

          try {
            console.log('ðŸŽ¤ Speech API í˜¸ì¶œ ì‹œìž‘')
            
            // APIë¡œ ì „ì†¡
            const response = await fetch('/api/speech-to-text', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify({ audio: base64Audio })
            })

            console.log('ðŸŽ¤ Speech API ì‘ë‹µ:', response.status)
            
            const data = await response.json()
            console.log('ðŸŽ¤ Speech API ë°ì´í„°:', data)

            if (response.ok && data.transcript) {
              console.log('âœ… ìŒì„± ì¸ì‹ ì„±ê³µ:', data.transcript)
              onResult(data.transcript)
            } else {
              console.error('âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨:', data.error || data.message)
              onError && onError(new Error(data.error || data.message || 'ìŒì„± ì¸ì‹ ì‹¤íŒ¨'))
            }
          } catch (error) {
            console.error('âŒ Speech-to-Text API ì˜¤ë¥˜:', error)
            onError && onError(error)
          }
        }

        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop())
          streamRef.current = null
        }
      }

      // ë…¹ìŒ ì‹œìž‘
      mediaRecorderRef.current.start()
      setIsListening(true)
      return true

    } catch (error) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì˜¤ë¥˜:', error)
      onError && onError(error)
      setIsListening(false)
      return false
    }
  }

  // ìŒì„± ì¸ì‹ ì‹œìž‘ - Web Speech API ìš°ì„  ì‚¬ìš©
  const startListening = () => {
    // ë¸Œë¼ìš°ì €ê°€ Web Speech APIë¥¼ ì§€ì›í•˜ë©´ ì‚¬ìš©
    if (typeof window !== 'undefined' && ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      if (!recognitionRef.current) {
        const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition
        recognitionRef.current = new SpeechRecognition()
        recognitionRef.current.continuous = false
        recognitionRef.current.interimResults = false
        recognitionRef.current.lang = 'ko-KR'

        recognitionRef.current.onresult = (event) => {
          const transcript = event.results[0][0].transcript
          if (transcript) {
            onResult(transcript)
          }
          setIsListening(false)
        }

        recognitionRef.current.onerror = (error) => {
          console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', error)
          setIsListening(false)
          onError && onError(error)
        }

        recognitionRef.current.onend = () => {
          setIsListening(false)
        }
      }
      
      try {
        recognitionRef.current.start()
        setIsListening(true)
        return true
      } catch (error) {
        console.error('ìŒì„± ì¸ì‹ ì‹œìž‘ ì˜¤ë¥˜:', error)
        onError && onError(error)
        return false
      }
    } else {
      // Web Speech API ë¯¸ì§€ì› ì‹œ Google Cloud ì‚¬ìš©
      return startListeningDesktop()
    }
  }

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopListening = () => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop()
        setIsListening(false)
      } catch (error) {
        console.error('ìŒì„± ì¸ì‹ ì¤‘ì§€ ì˜¤ë¥˜:', error)
      }
    } else if (mediaRecorderRef.current && isListening) {
      mediaRecorderRef.current.stop()
      setIsListening(false)
    }
  }

  // ìŒì„± ì¸ì‹ í† ê¸€
  const toggleListening = () => {
    if (isListening) {
      stopListening()
      return false
    } else {
      return startListening()
    }
  }

  return {
    startListening,
    stopListening,
    toggleListening,
    isListening
  }
}
